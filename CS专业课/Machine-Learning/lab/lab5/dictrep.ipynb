{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5: LDA Topic Model with Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glimpse of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well i'm not sure about the story nad it did seem biased. What\n",
      "I disagree with is your statement that the U.S. Media is out to\n",
      "ruin Israels reputation. That is rediculous. The U.S. media is\n",
      "the most pro-israeli media in the world. Having lived in Europe\n",
      "I realize that incidences such as the one described in the\n",
      "letter have occured. The U.S. media as a whole seem to try to\n",
      "ignore them. The U.S. is subsidizing Israels existance and the\n",
      "Europeans are not (at least not to the same degree). So I think\n",
      "that might be a reason they report more clearly on the\n",
      "atrocities.\n",
      "\tWhat is a shame is that in Austria, daily reports of\n",
      "the inhuman acts commited by Israeli soldiers and the blessing\n",
      "received from the Government makes some of the Holocaust guilt\n",
      "go away. After all, look how the Jews are treating other races\n",
      "when they got power. It is unfortunate.\n",
      "\n",
      "There even exists null text:   \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "raw_text_list = np.load(\"text.npy\")\n",
    "print(raw_text_list[0])\n",
    "print('There even exists null text: ', raw_text_list[51])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'text'\n",
    "# dataset_name = 'computer-cat'\n",
    "if dataset_name == 'text':\n",
    "    raw_text_list = np.load(\"text.npy\")\n",
    "    num_topics = 20\n",
    "    num_keywords = 10\n",
    "    alpha = np.ones(num_topics)\n",
    "    eta = None  # TO_BE_ASSIGNED\n",
    "elif dataset_name == 'computer-cat':\n",
    "    raw_text_list = np.load(\"computer-cat.npy\")\n",
    "    num_topics = 2\n",
    "    num_keywords = 5\n",
    "    alpha = np.ones(num_topics)\n",
    "    eta = None  # TO_BE_ASSIGNED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess\n",
    "\n",
    "1. filter strange words, here `sklearn.feature_extraction.text.CountVectorizer` or `nltk.*` is used.\n",
    "1. get the dictionary\n",
    "2. transform texts into `Doc` object, which contains the `freq_dist` and the `words` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "punctuations = {'(', ')', '{', '}', '[', ']', '\"', \"'\",\n",
    "                ',', ';', '.', '!', '?'}\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "exception_search_pattern = re.compile(r'\\^+|=+|~+|-\\|*-|_+|\\|_*\\|')\n",
    "exception_match_pattern = re.compile(r'^[\\']?[-+]?([0-9]+(\\.[0-9]+)?|\\.[0-9]+)|(/\\\\)+')\n",
    "should_search_pattern = re.compile(r'[\\w]+')\n",
    "brown_taged = dict(nltk.corpus.brown.tagged_words())\n",
    "\n",
    "count = 0\n",
    "def pretransform(word):\n",
    "    global count\n",
    "    ok = word not in stopwords_set and word not in punctuations \\\n",
    "        and not exception_search_pattern.search(word) and not exception_match_pattern.match(word)\n",
    "    ok = ok and should_search_pattern.search(word)\n",
    "    if not ok:\n",
    "        return ''\n",
    "    if word[0] == '-' or word[0] == '|':\n",
    "        word = word[1:]\n",
    "    if word in brown_taged and brown_taged[word] != 'NN':\n",
    "        word = ''\n",
    "    return word\n",
    "\n",
    "class Doc:\n",
    "    def __init__(self, text, count_vectorizer: CountVectorizer, tokenizer='sklearn'):\n",
    "        # 1. get tokens by `sklearn.feature_extraction.text.CountVectorizer` or by `nltk.word_tokenize()`\n",
    "        if tokenizer == 'sklearn':\n",
    "            words_vec = count_vectorizer.transform([text]).toarray()[0]\n",
    "            dictionary = count_vectorizer.get_feature_names()\n",
    "            wids = np.where(words_vec > 0)[0]\n",
    "            words = []\n",
    "            for wid in wids:\n",
    "                words += [dictionary[wid] for _ in range(words_vec[wid])]\n",
    "        else:\n",
    "            assert tokenizer == 'nltk' and 'Please use `sklearn` or `nltk` as tokenizer.'\n",
    "            words = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "        # 2. pretransform words with `pretransform` to filter useless words or turn words into better forms\n",
    "        words = [pretransform(word) for word in words]\n",
    "        # 3. filter all `''` (null string)\n",
    "        words = [word for word in words if word != '']\n",
    "        # 4. set corresponding variables\n",
    "        self.text = text\n",
    "        self.freq_dist = dict(nltk.FreqDist(words))\n",
    "        self.words = np.array(sorted(list(self.freq_dist)))\n",
    "        self.num_words = len(self.words)\n",
    "        self.last_assigned_topics = np.zeros(self.words.shape)\n",
    "        self.assigned_topics = np.zeros(self.words.shape)\n",
    "        self.word_ids = None\n",
    "        self.wid_freq_dist = None\n",
    "        \n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, text_list, stopwords_set, num_topics):\n",
    "        self.text_list = text_list\n",
    "        self.stopwords_set = set(stopwords.words('english'))\n",
    "        self.doc_list = self.preprocess()\n",
    "        self.dictionary = self.get_dictionary()\n",
    "        self.num_topics = num_topics\n",
    "        self.num_docs = len(self.doc_list)\n",
    "        self.num_words = len(self.dictionary)\n",
    "        # wid here means word id\n",
    "        self.word_to_wid = {self.dictionary[i]: i for i in range(self.num_words)}\n",
    "        self.set_word_ids_and_freq()\n",
    "        \n",
    "    def set_word_ids_and_freq(self):\n",
    "        for doc in self.doc_list:\n",
    "            doc.word_ids = np.array([self.word_to_wid[word] for word in doc.words])\n",
    "            doc.wid_freq_dist = Counter(doc.word_ids)\n",
    "        \n",
    "    def get_dictionary(self):\n",
    "        dictionary = set()\n",
    "        for doc in self.doc_list:\n",
    "            dictionary |= set(doc.words)\n",
    "        return np.array(sorted(list(dictionary)))\n",
    "    \n",
    "    def preprocess(self):\n",
    "        count_vectorizer = CountVectorizer()\n",
    "        count_vectorizer.fit(raw_text_list)\n",
    "        doc_list = []\n",
    "        for text in self.text_list:\n",
    "            doc = Doc(text, count_vectorizer)\n",
    "            if doc.num_words > 0:\n",
    "                doc_list.append(doc)\n",
    "        return doc_list\n",
    "    \n",
    "    def count(self):\n",
    "        n_word = np.zeros((self.num_topics, self.num_words))\n",
    "        n_topic = np.zeros((self.num_docs, self.num_topics))\n",
    "        # TODO: this could be paralleled, cuda or multiprocessing\n",
    "        for doc_id, doc in enumerate(self.doc_list):\n",
    "            for wid_in_doc, word in enumerate(doc.words):\n",
    "                word_freq = doc.freq_dist[word]\n",
    "                topic_id = doc.assigned_topics[wid_in_doc]\n",
    "                wid = self.word_to_wid[word]\n",
    "                n_word[topic_id, wid] += 1 #word_freq\n",
    "                n_topic[doc_id, topic_id] += 1 # word_freq\n",
    "        return n_word, n_topic\n",
    "        \n",
    "        \n",
    "dataset = Dataset(raw_text_list, stopwords_set=stopwords_set, num_topics=num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austria' 'biased' 'blessing' 'commited' 'degree' 'europe' 'europeans'\n",
      " 'existance' 'government' 'guilt' 'holocaust' 'incidences' 'israeli'\n",
      " 'israels' 'jews' 'letter' 'nad' 'occured' 'power' 'reason' 'rediculous'\n",
      " 'reputation' 'statement' 'story' 'subsidizing' 'world']\n",
      "{'austria': 1, 'biased': 1, 'blessing': 1, 'commited': 1, 'degree': 1, 'europe': 1, 'europeans': 1, 'existance': 1, 'government': 1, 'guilt': 1, 'holocaust': 1, 'incidences': 1, 'israeli': 2, 'israels': 2, 'jews': 1, 'letter': 1, 'nad': 1, 'occured': 1, 'power': 1, 'reason': 1, 'rediculous': 1, 'reputation': 1, 'statement': 1, 'story': 1, 'subsidizing': 1, 'world': 1}\n",
      "970\n",
      "(11455,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.doc_list[0].words)\n",
    "print(dataset.doc_list[0].freq_dist)\n",
    "print(dataset.num_docs)\n",
    "print((dataset.dictionary.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA and Gibbs Sampling\n",
    "\n",
    "### Gibbs Sampling in LDA\n",
    "\n",
    "Here is the corresponding conditional probability:\n",
    "$$P(z_i=k'|z_{-i},\\alpha,\\eta)\\propto\\frac{\\eta_{v'}+n_{k'v'}}{\\sum_v\\eta_v+n_{k'v}}\\cdot \\frac{\\alpha_{k'}+n_{d'k'}}{\\sum_k\\alpha_k+n_{d'k}}$$\n",
    "\n",
    "Then just follow the algorithm below, we can get all $\\textbf{z}_i$\n",
    "\n",
    "```python\n",
    "for i in range(T):\n",
    "    do gibbs sample\n",
    "```\n",
    "\n",
    "### Get $\\theta_d$ and $\\beta_k$\n",
    "\n",
    "$\\theta_d$ and $\\beta_k$ can be updated with this estimation:\n",
    "$$\\theta_{dk}=\\frac{n_{dk}+\\alpha_k}{\\sum_k(n_{dk}+\\alpha_k)},\\qquad\\beta_{kv}=\\frac{n_{kv}+\\eta_v}{\\sum_v(n_{kv}+\\eta_v)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "epoch 0 0.9517414601473543 970.0 19.999999999999996\n",
      "epoch 1 0.882786336235767 325.2585051199026 2.8584646290876643\n",
      "epoch 2 0.8782652377762894 328.49268231225307 2.8348924645752906\n",
      "epoch 3 0.8749162759544541 322.23937240852734 2.8298618186197064\n",
      "epoch 4 0.8798727394507703 322.86276699324105 2.844087969684036\n",
      "epoch 5 0.87357669122572 323.52736159271797 2.8290981567617597\n",
      "epoch 6 0.8767916945746819 325.61418716205395 2.8334027202826233\n",
      "epoch 7 0.8725050234427327 324.5473170140963 2.8159619801274474\n",
      "epoch 8 0.872906898861353 326.9642061281044 2.832406756978396\n",
      "epoch 9 0.8790689886135298 322.6309011650695 2.8263298708158358\n",
      "epoch 10 0.875753516409913 326.40144605067326 2.8378613717245735\n",
      "epoch 11 0.8752176825184192 327.25252094245474 2.8427030043318715\n",
      "epoch 12 0.871868720696584 322.2806366946076 2.830711749596029\n",
      "epoch 13 0.8764567983924983 328.97194910658385 2.816756689041937\n",
      "epoch 14 0.8721701272605492 324.687502057449 2.830820316174429\n",
      "epoch 15 0.8730073677160081 323.44869627472536 2.8504083379678087\n",
      "epoch 16 0.8753516409912927 325.7354101361838 2.8314249933968076\n",
      "epoch 17 0.8757200267916946 325.6723737503958 2.850344048983181\n",
      "epoch 18 0.877662424648359 324.9116402839428 2.8468357324132785\n",
      "epoch 19 0.8761219022103148 324.61436313139376 2.8437335371366417\n",
      "epoch 20 0.8745813797722706 324.12069085784816 2.8237727733712723\n",
      "epoch 21 0.8721031480241125 326.22789140439664 2.8284971809436317\n",
      "epoch 22 0.8738780977896852 326.16024339917965 2.825768705207183\n",
      "epoch 23 0.868821165438714 325.58998788921787 2.8147590849028683\n",
      "epoch 24 0.8735432016075017 328.92188904033736 2.848330398925272\n",
      "epoch 25 0.8757200267916946 327.4407272412627 2.8206870252928433\n",
      "epoch 26 0.8762893503014065 325.6092532090944 2.8286766941076364\n",
      "epoch 27 0.87337575351641 327.19849665664395 2.8293887880219124\n",
      "epoch 28 0.8708305425318151 328.46978648926313 2.8478959511260027\n",
      "epoch 29 0.8734427327528466 330.0417721192846 2.847466429895171\n",
      "epoch 30 0.8695914266577361 325.19529107537545 2.8323009912660684\n",
      "epoch 31 0.8751507032819826 331.23133955169885 2.83581280635799\n",
      "epoch 32 0.8742129939718687 326.57934221995447 2.827140280608921\n",
      "epoch 33 0.8719691895512391 324.54047270579343 2.8148512786963296\n",
      "epoch 34 0.874715338245144 323.0498217616209 2.8401591528951355\n",
      "epoch 35 0.8754521098459478 327.2557530041572 2.8273583881115867\n",
      "epoch 36 0.8720026791694575 324.3923963605544 2.818312964385722\n",
      "epoch 37 0.8710314802411252 322.2619492363194 2.8053576991428137\n",
      "epoch 38 0.8681178834561286 327.9594793691371 2.8275972918800374\n",
      "epoch 39 0.8721701272605492 327.0346933709709 2.8259776280025717\n",
      "epoch 40 0.8744474212993972 328.5907419981274 2.835362948926647\n",
      "epoch 41 0.8722036168787676 323.7876456872481 2.8296963359534306\n",
      "epoch 42 0.8704286671131949 326.8986294183669 2.8332410524094693\n",
      "epoch 43 0.8751172136637643 324.4532470894387 2.836667183887749\n",
      "epoch 44 0.874045545880777 330.7703313468942 2.833947114312521\n",
      "epoch 45 0.871198928332217 323.2904879622132 2.8263999409419975\n",
      "epoch 46 0.8763228399196249 329.70586885713226 2.8350152221920255\n",
      "epoch 47 0.8738780977896852 324.1453253836804 2.8279168950839546\n",
      "epoch 48 0.8703951774949765 325.6497762166972 2.8254904389977193\n",
      "epoch 49 0.8745478901540522 329.1480974506775 2.8375055733711374\n",
      "epoch 50 0.8714668452779638 324.1899403299923 2.8255825654064024\n",
      "epoch 51 0.8689886135298057 327.8163278179137 2.809643661201189\n",
      "epoch 52 0.875853985264568 323.8058644476319 2.82562821546212\n",
      "epoch 53 0.8749162759544541 329.72810986355546 2.8129928251495966\n",
      "epoch 54 0.8736436704621567 323.72189748159485 2.792464580107553\n",
      "epoch 55 0.8725385130609511 328.1711402914949 2.843676104685714\n",
      "epoch 56 0.8753181513730743 325.8614692312022 2.8214529673139306\n",
      "epoch 57 0.874146014735432 328.54363880901803 2.820042430991369\n",
      "epoch 58 0.8736101808439384 327.0060120555693 2.816558228495845\n",
      "epoch 59 0.875284661754856 325.4774351998564 2.827545217445005\n",
      "epoch 60 0.8688546550569324 323.34954621018784 2.825210319409164\n",
      "epoch 61 0.8721701272605492 321.725056314257 2.8311266267226736\n",
      "epoch 62 0.8712324179504354 326.3752187302326 2.8256016564280575\n",
      "epoch 63 0.8700267916945746 324.55116119112984 2.78412496859149\n",
      "epoch 64 0.8726054922973878 325.37207972167994 2.830908841137272\n",
      "epoch 65 0.873107836570663 322.027752338473 2.830222524244032\n",
      "epoch 66 0.8733422638981916 324.2160392553476 2.82875622773649\n",
      "epoch 67 0.8727394507702613 320.9938555796734 2.8218845647412105\n",
      "epoch 68 0.871868720696584 323.68067807685685 2.809474803744463\n",
      "epoch 69 0.8741125251172137 328.4432428189869 2.832534357491457\n",
      "epoch 70 0.8729403884795713 327.86499289662913 2.815018021715342\n",
      "epoch 71 0.8713998660415271 326.90423723458605 2.849652919408911\n",
      "epoch 72 0.8763228399196249 330.8352532751486 2.833650172196612\n",
      "epoch 73 0.874045545880777 321.1168669163367 2.819938677244556\n",
      "epoch 74 0.8758874748827863 325.8660140312526 2.8300454181660077\n",
      "epoch 75 0.8739115874079035 327.5450052359136 2.838684476542156\n",
      "epoch 76 0.8753516409912927 324.233964790028 2.840149072233584\n",
      "epoch 77 0.870060281312793 321.32756988367265 2.8279265882192757\n",
      "epoch 78 0.8686872069658406 325.3719044202555 2.801468713152114\n",
      "epoch 79 0.8716008037508373 327.4979428413458 2.802229517339187\n",
      "epoch 80 0.8698258539852646 322.8042048527522 2.818232037256403\n",
      "epoch 81 0.8667448091091762 326.9188597337674 2.7976829683842723\n",
      "epoch 82 0.8690555927662424 323.7591015967078 2.8087607647090516\n",
      "epoch 83 0.87073007367716 331.4734168119413 2.7964254523848746\n",
      "epoch 84 0.8720026791694575 326.5293583454101 2.837198057192703\n",
      "epoch 85 0.868151373074347 325.33230780340193 2.8088777045287734\n",
      "epoch 86 0.8724045545880776 326.55707067724177 2.829452458217859\n",
      "epoch 87 0.8726054922973878 328.7987727073728 2.813169796776672\n",
      "epoch 88 0.8713328868050905 323.735206524725 2.820958157011301\n",
      "epoch 89 0.8709310113864702 328.2123211419724 2.820312768980378\n",
      "epoch 90 0.8721366376423308 327.710124164146 2.8045174620836155\n",
      "epoch 91 0.8719691895512391 325.98958371888 2.8087150593524606\n",
      "epoch 92 0.8731748158070998 329.9028784587211 2.813162050239799\n",
      "epoch 93 0.8702612190221032 320.5441415361905 2.823911331359655\n",
      "epoch 94 0.8728734092431346 323.1516786440374 2.8143869191062705\n",
      "epoch 95 0.8725720026791695 329.6362120147526 2.8321998629217315\n",
      "epoch 96 0.8698928332217013 324.3362742991321 2.81708003028086\n",
      "epoch 97 0.8756195579370395 323.9743571753102 2.834090943368764\n",
      "epoch 98 0.8707970529135968 322.5114084001758 2.820286079979767\n",
      "epoch 99 0.8726054922973878 325.31876719568925 2.816851771137745\n",
      "epoch 100 0.8741125251172137 327.36737634214563 2.8280890367004337\n",
      "epoch 101 0.8737106496985935 327.00924366560986 2.809415688866061\n",
      "epoch 102 0.8691225720026792 322.230604103287 2.824368787357356\n",
      "epoch 103 0.8706965840589417 332.6119325453203 2.831315479100866\n",
      "epoch 104 0.8726389819156062 329.1964710386584 2.8083528180564636\n",
      "epoch 105 0.87337575351641 324.1170870048174 2.838287862720593\n",
      "epoch 106 0.872438044206296 319.52957982465074 2.80473029580407\n",
      "epoch 107 0.8733087742799732 327.077671417563 2.8161753442734594\n",
      "epoch 108 0.872906898861353 325.8188319816391 2.8226019477345496\n",
      "epoch 109 0.8688546550569324 325.54606978809323 2.821457681147092\n",
      "epoch 110 0.8703281982585398 320.96882509616626 2.800480829523276\n",
      "epoch 111 0.8726054922973878 328.69268290710414 2.818904307160128\n",
      "epoch 112 0.87337575351641 325.4982240045999 2.8280609836115027\n",
      "epoch 113 0.8726054922973878 319.67610136587666 2.827200448779113\n",
      "epoch 114 0.875954454119223 325.5486989141547 2.842287816432709\n",
      "epoch 115 0.8736101808439384 332.5732019240364 2.837872163981425\n",
      "epoch 116 0.8720026791694575 328.26565409916253 2.8176277090942494\n",
      "epoch 117 0.8745478901540522 326.41903336430187 2.8352861186527623\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-aa9509820092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_keywords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_keywords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epoches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-aa9509820092>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, dataset, max_epoches)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mdoc_eta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mdoc_wid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                     \u001b[0mdoc_n_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mtopic_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[0mpz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_wid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdoc_eta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_wid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_eta\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdoc_n_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class LDA:\n",
    "    def __init__(self, num_topics, num_keywords, alpha, eta):\n",
    "        self.num_topics = num_topics\n",
    "        self.num_keywords = num_keywords\n",
    "        self.alpha = np.array(alpha)\n",
    "        self.eta = np.array(eta)\n",
    "    def train(self, dataset: Dataset, max_epoches = 1):\n",
    "        # Assign topic to each word randomly\n",
    "        for doc in dataset.doc_list:\n",
    "            doc.assigned_topics = np.random.randint(low=0, high=self.num_topics-1, size=doc.assigned_topics.shape)\n",
    "        # Gibbs Sampling\n",
    "        n_word, n_topic = dataset.count()\n",
    "        \n",
    "        last_theta = np.zeros((dataset.num_docs, dataset.num_topics))\n",
    "        last_beta = np.zeros((dataset.num_topics, dataset.num_words))\n",
    "        last_error = np.inf\n",
    "        print('=====================================================================================')\n",
    "        for epoch_id in range(max_epoches):\n",
    "            for doc_id, doc in (enumerate(dataset.doc_list)):\n",
    "                # get the conditional probability for this doc, i.e. `pz`\n",
    "                pz = np.zeros((doc.num_words, dataset.num_topics))\n",
    "                doc_eta = self.eta[doc.word_ids]\n",
    "                for doc_wid, wid in enumerate(doc.word_ids):\n",
    "                    doc_n_word = n_word[:, doc.word_ids]\n",
    "                    for topic_id in range(dataset.num_topics):\n",
    "                        pz[doc_wid][topic_id] = (doc_eta[doc_wid] + n_word[topic_id, wid]) / np.sum(doc_eta + doc_n_word[topic_id]) \\\n",
    "                                                * (self.alpha[topic_id] + n_topic[doc_id, topic_id]) / np.sum(self.alpha + n_topic[doc_id])\n",
    "                # update the topic assignment to each word, i.e. `doc.assigned_topics`\n",
    "                for doc_wid in range(doc.num_words):\n",
    "                    # wzh's sampling\n",
    "                    doc.assigned_topics[doc_wid] = np.argmax(np.random.multinomial(1, pz[doc_wid]/pz[doc_wid].sum(), size=1))\n",
    "            # calculate theta and beta, to see whether they are convergent\n",
    "            n_word, n_topic = dataset.count()\n",
    "            theta = self.get_theta(dataset, n_topic)\n",
    "            beta = self.get_beta(dataset, n_word)\n",
    "            \n",
    "            total_count_words = 0\n",
    "            still_pred = 0\n",
    "            for doc_id, doc in enumerate(dataset.doc_list):\n",
    "                still_pred += np.sum((doc.assigned_topics - doc.last_assigned_topics) != 0)\n",
    "                total_count_words += doc.assigned_topics.shape[0]\n",
    "                doc.last_assigned_topics = doc.assigned_topics.copy()\n",
    "            error = still_pred / total_count_words\n",
    "            \n",
    "            if epoch_id % 1 == 0:\n",
    "#                 print(self.get_topics(dataset))\n",
    "                print(f'epoch {epoch_id}', error, np.abs(last_theta - theta).sum(), np.abs(last_beta - beta).sum())\n",
    "#                 print('=====================================================================================')\n",
    "#             if 0 < np.abs(last_error - error) < 1e-7:\n",
    "#                 break\n",
    "            last_theta = theta\n",
    "            last_beta = beta\n",
    "            last_error = error\n",
    "            \n",
    "    def get_theta(self, dataset, n_topic):\n",
    "        theta = np.zeros((dataset.num_docs, dataset.num_topics))\n",
    "        for doc_id, doc in enumerate(dataset.doc_list):\n",
    "            theta[doc_id] = (n_topic[doc_id] + self.alpha) / np.sum(n_topic[doc_id] + self.alpha)\n",
    "        return theta\n",
    "    \n",
    "    def get_beta(self, dataset, n_word):\n",
    "        beta = np.zeros((dataset.num_topics, dataset.num_words))\n",
    "        for topic_id in range(dataset.num_topics):\n",
    "            beta[topic_id] = (n_word[topic_id] + self.eta) / np.sum(n_word[topic_id] + self.eta)\n",
    "        return beta\n",
    "    \n",
    "    def get_topics(self, dataset, topk=10):\n",
    "        n_word, n_topic = dataset.count()\n",
    "        beta = self.get_beta(dataset, n_word)\n",
    "        top_wid = np.argsort(-beta)[:, :topk]\n",
    "        topics = dataset.dictionary[top_wid]\n",
    "        return topics\n",
    "    \n",
    "from datetime import datetime\n",
    "np.set_printoptions(linewidth=120)\n",
    "eta = np.ones(dataset.num_words)\n",
    "lda = LDA(num_topics=num_topics, num_keywords=num_keywords, alpha=alpha, eta=eta)\n",
    "start = datetime.now()\n",
    "lda.train(dataset, max_epoches=1000)\n",
    "print(datetime.now() - start)\n",
    "print(lda.get_topics(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
