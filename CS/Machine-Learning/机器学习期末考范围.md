这我们整个这个书从第一章讲到第11章，12没讲，13，14讲了
重点内容肯定都是在前面

- [ ] 第 2 个是关于评价, 就是模型的评估和评价. 期中考试考过了, 所以 `可能`(重音) 题会不太一样, 如何评估一个算法, 如何对比算法的优劣性.

- [x] "第一章肯定不考了"我们只希望大家能学会这种方法(可能写在黑板上了)

- [ ] 第 3 章是线性模型, 主要关注 `对率回归(LR)`, 是必考内容, LDA 重要性没有前面那个大. LR 要知道目标函数的定义, 怎么优化, 导数怎么求, 算法过程怎样.

- [ ] 第 4 章是决策树, 期中考考过了分类树, 这种类型的肯定不会再考了, 但 `XGBoost` 里有一种新的方法, 用的是回归树, 这个方法要注意一下, 分类问题同样可以用回归树去解它. 要注意一下这个内容如何变幻到这里. 如何用目标函数直接去定义出我们的决策树来.

- [x] 第 5 部分是神经网络, 重点就是求梯度, 就是所谓 BP 算法, 以及参数更新的过程. 其他的像模型结构, 损失函数的定义这个我们在 ???(05:25) 已经讲过了, 那么最重要的是我们的 ???(05:28)

- [x] 第 6 章是 SVM, 这章内容很重要, SVR 不考, 大家都记不住, 但是 SVM 会考. SVM 整个过程要很熟悉, 跟前面 逻辑回归是一样的, 要知道这类模型损失函数是什么, 如何优化它, 终止条件也要知道, 这些所有的细节都要知道.

- [x] 第 7 章内容是 `贝叶斯`, 这部分内容主要考 EM 算法, 贝叶斯不考太简单了. EM 算法主要和变分法结合在一起考, 把贝叶斯和第 14 章放在一起考, 大概会有一个类似于 EM 算法或者是变分推断的. 这个变分法而且还是确定性的哈. 
    - 判别模型和生成模型

- [x] 第 8 章是集成学习主要是 `AdaBoost`, `XGBoost` 不会在这里出了, 前面会出. 所有公式都会写在试卷上.
    - 178页 $h_t=\mathcal{L}(\mathcal{D}, \mathcal{D}_{bs})$
    - 降低方差和降低偏差有什么含义

- [x] 第 9 章聚类, 主要考 `K-Means`, 密度聚类肯定不考, 麻烦. 还有就是 `高斯混合模型`.

- [x] 第 10 章降维, `PCA` 为主, 可能还会有 `度量学习(ML)`

- [ ] 第 11 章可能会和 `LR` 在一起考, `Sparse Coding` 不考, 这主要是考 特征选择(12:25)

- [ ] 第 13 章半监督学习这块没有出习题, 这个... 13章会考 图半监督??(13:25)

- [ ] 第 14 章主要是, 再考一下隐马尔可夫模型, 剩下的相对比较复杂, 就不考了. 主题模型做过实验就不考. 但是变分推断我们就会考, 14章非常重要.

